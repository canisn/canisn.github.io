---
layout: post
title:  "CTR预估之线性模型!"
categories: 推荐系统
tags:  CTR LR GBDT 
author: canisn
---

* content
{:toc}

线性CTR预估模型




# LR(Logistic Regerssion) 线性模型之根本
没啥说的，必备科目。

# LS-PLM(Large Scale Piece-wise Linear Model) 阿里妈妈出品
LS-PLM(也叫作 MLR, Mixture of Logistics Regression)是阿里妈妈在 2017 年在论文 [Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction](https://arxiv.org/pdf/1704.05194.pdf) 中公开的，但是模型早在 2012 年就在阿里妈妈内部使用。这个模型在 LR 基础上进行了拓展，目的是为了解决单个 LR 无法进行非线性分割的问题。

# GBDT+LR(Gradient Boost Decision Tree + Logistic Regression) facebook出品
GBDT + LR 是 FaceBook 在这篇论文 [Practical Lessons from Predicting Clicks on Ads at Facebook](http://quinonero.net/Publications/predicting-clicks-facebook.pdf) 中提出的，其思想是借助 GBDT 帮我们做部分特征工程，然后将 GBDT 的 输出作为 LR 的输入。

我们前面提到的无论LR还是MLR，都避免不了要做大量的特征工程。比如说构思可能的特征，将连续特征离散化，并对离散化的特征进行One-Hot编码，最后对特征进行二阶或者三阶的特征组合/交叉，这样做的目的是为了得到非线性的特征。但是特征工程存在几个难题：
- 连续变量切分点如何选取？
- 离散化为多少份合理？
- 选择哪些特征交叉？
- 多少阶交叉，二阶，三阶或更多？

而 GBDT + LR 这个模型中，GBDT 担任了特征工程的工作。

GBDT中采用的决策树是CART树，关于决策树的介绍可参考这篇文章[ID3/C4.5/CART算法比较](http://www.cnblogs.com/wxquare/p/5379970.html)。
记录一下要点：
- ID3

分类依据是“最大信息熵增益”，枚举选举最佳分类特征并在特征上根据取值个数一次完全切分。问题是类别多的特征更有优势，容易被选取。不能处理连续值。
- C4.5

分类依据是“信息增益比”， 并引入“分裂信息”作为惩罚项，可以处理连续值。
- CART

可以做分类和回归，每次二分类切必须二分类。分类时使用基尼指数（Gini），回归时使用均方差。

- 分类树与回归树区别

分类是采用找阈值求最大熵最为切分点。

回归树每个节点有一个预测值，表示这个节点上该特征的平均取值，计算节点均方误差最小化就可以找到最合适的切分点。最后叶子节点的该特征均值就是预测值。

下面回来介绍 GBDT， 这里决策时采用CART回归树，切分方法是：

枚举每一个特征的每一个切分，将节点切分为左右子树，计算左右子树在该特征上的平均值，计算左右子树的误差和平均值的和，最后比较全部的误差均值取最小的为当前的最右切分点。

以上就是GBDT的“DT”部分，GB部分就是一个提升过程。

![梯度提升树提升过程](https://note.youdao.com/yws/public/resource/c1f1386dc9a34b3fbeaede7a5d44ad9e/xmlnote/WEBRESOURCE106c1e1bae9f2b1bedd233e52799e2a4/271)

上图就是表示提升过程中参数的更新，具体更新方法是每次得到一个分类器后，对于正确分类的样本，减小该样本的权重，同时增大错误分类样本的权重，重新进行切分点计算得到一颗新的树，不断迭代得到一系列的树。最后按照分类的正确率，正确率搞得权重大，加权叠加就得到最终的分类或者回归模型。GBDT的更新是一个比较复杂的过程，可以[参考文章](https://blog.csdn.net/google19890102/article/details/51746402)。

![梯度提升树用于LR参数初始化](https://note.youdao.com/yws/public/resource/c1f1386dc9a34b3fbeaede7a5d44ad9e/xmlnote/3F0A331BDCF54E8EBC196407F072660C/284)

GBDT+LR 方案中每棵决策树从根节点到叶节点的路径，会经过不同的特征，此路径就是特征组合，而且包含了二阶，三阶甚至更多，因此输出的 one-hot 特征是原始特征进行交叉后的结果。而且每一维的特征其实还是可以追溯出其含义的，因为从根节点到叶子节点的路径是唯一的，因此落入到某个叶子节点表示这个特征满足了这个路径中所有节点判断条件。

**几个问题**
GBDT+LR不适用于超高维稀疏特征，比如用户ID，商品ID这样上千万级特征。解决办法通常是这类特征不经过GBDT，而是直接one-hot后连同其他编码特征一起输入到LR中。

另一个问题是GBDT一般用于处理连续特征，对于离散特征有两种做法：
1. 离散特征不直接输入到GBDT中进行编码，而是做one-hot编码后直接输入到LR中即可；对于连续特征，先通过GBDT进行离散化和特征组合输出one-hot编码的特征，最后结合这两种one-hot特征直接输入到LR。大致框架如下所示
    ![高维稀疏特征与GBDT混合模型](https://note.youdao.com/yws/public/resource/c1f1386dc9a34b3fbeaede7a5d44ad9e/xmlnote/A0C3A68B608E47D68DD13B80D5B60CF3/301)

2. 将离散的特征也输入GBDT进行编码，但是只保留那些出现频率高的离散特征，这样输入 GBDT 中的 one-hot 特征的维度会遍地，同时通过 GBDT 也对原始的 one-hot 特征进行了组合和交叉。