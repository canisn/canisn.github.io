---
layout: post
title:  "Hadoop相关"
categories: hadoop
tags:  linux python
author: canisn
---

* content
{:toc}




# 1. hadoop_streaming
Hadoop Streaming提供了一个便于进行MapReduce编程的工具包，使用它可以基于一些可执行命令、脚本语言或其他编程语言来实现Mapper和Reducer，从而充分利用Hadoop并行计算框架的优势和能力，来处理大数据。

在官网下载hadoop二进制包部署完成后，hadoop-streaming已经内建在hadoop-2.6.0/share/hadoop/tools/lib/中了,具体路径跟发行版本有关。

接下来是选择大数据统计的样本，随便找一点数据集测试就行了。比如天池数据，地址https://tianchi.shuju.aliyun.com/datalab/index.htm

数据是一个csv文件，结构如下：

用户名,出生日期,性别（0女，1男，2不愿意透露性别）

比如：415971,20121111,0（数据已经脱敏处理）

下面我们来试着统计每年的男女婴人数


```shell

#!/usr/bin/python
# -*- coding: utf-8 -*-
# ./mapper

import sys

for line in sys.stdin:
    line = line.strip()
    data = line.split(',')
    if len(data)<3:
        continue
    user_id = data[0]
    birthyear = data[1][0:4]
    gender = data[2]
    print >>sys.stdout,"%s\t%s"%(birthyear,gender)
```

下面是reduce程序，这里大家需要注意一下，map到reduce的期间，hadoop会自动给map出的key排序，所以到reduce中是一个已经排序的键值对，这简化了我们的编程工作


```shell
#!/usr/bin/python
# -*- coding: utf-8 -*-

import sys

gender_totle = {'0':0,'1':0,'2':0}
prev_key = False
for line in sys.stdin:#map的时候map中的key会被排序
    line = line.strip()    
    data = line.split('\t')
    birthyear = data[0]
    curr_key = birthyear
    gender = data[1]
    
    #寻找边界，输出结果
    if prev_key and curr_key !=prev_key:#不是第一次，并且找到了边界
        print >>sys.stdout,"%s year has female %s and male %s"%(prev_key,gender_totle['0'],gender_totle['1'])#先输出上一次统计的结果
        prev_key = curr_key
        gender_totle['0'] = 0
        gender_totle['1'] = 0
        gender_totle['2'] = 0#清零
        gender_totle[gender] +=1#开始计数
    else:
        prev_key = curr_key
        gender_totle[gender] += 1
#输出最后一行
if prev_key:
    print >>sys.stdout,"%s year has female %s and male %s"%(prev_key,gender_totle['0'],gender_totle['1'])
```
接下来就是将样本和mapper，reducer上传到hdfs中并执行了，可以先在本地测试下python脚本是否正确

```
cat sample.csv | ./mapper.py | sort -t ' ' -k 1 | ./reducer.py 
```
将样本上传到HDFS上
```
dfs -mkdir -p /user/root/input
dfs -put ./sample.csv /user/root/input
```
接下来将mapper.py和reducer.py上传到服务器上

```shell
hadoop jar /usr/local/hadoop/hadoop-streaming-0.23.6.jar \
-D mapred.job.name="testhadoop" \
-D mapred.job.queue.name=testhadoopqueue \
-D mapred.map.tasks=50 \
-D mapred.min.split.size=1073741824 \
-D mapred.reduce.tasks=10 \
-D stream.num.map.output.key.fields=1 \
-D num.key.fields.for.partition=1 \
-input input/sample.csv \
-output output-streaming \
-mapper mapper.py \
-reducer reducer.py \
-file mapper.py \
-file reducer.py \
-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
```

命令的解释如下

1. -input：输入文件路径
2. -output：输出文件路径
3. -mapper：用户自己写的mapper程序，可以是可执行文件或者脚本
4. -reducer：用户自己写的reducer程序，可以是可执行文件或者脚本
5. -file：打包文件到提交的作业中，可以是mapper或者reducer要用的输入文件，如配置文件，字典等。这个一般是必须有的，因为mapper和reducer函数都是写在本地的文件中，因此需要将文件上传到集群中才能被执行
6. -partitioner：用户自定义的partitioner程序
7. -D：作业的一些属性（以前用的是-jonconf），具体有：
- mapred.map.tasks：map task数目设置的数目与实际运行的值并不一定相同，若输入文件含有M个part，而此处设置的map_task数目超过M，那么实际运行map_task仍然是M
- mapred.reduce.tasks：reduce task数目  不设置的话，默认值就为1
- num.key.fields.for.partition=N：shuffle阶段将数据集的前N列作为Key；所以对于wordcount程序，map输出为“word  1”，shuffle是以word作为Key，因此这里N=1
8. -D stream.num.map.output.key.fields=1 这个是指在reduce之前将数据按前1列做排序，一般情况下可以去掉

如果出现以下字样就是成功了

```shell
16/08/18 18:35:20 INFO mapreduce.Job:  map 100% reduce 100%
16/08/18 18:35:20 INFO mapreduce.Job: Job job_local926114196_0001 completed successfully
```
之后使用如下命令将结果取回本地，使用cat命令就能查看

```shell
dfs -get /user/root/output-streaming/* ./output-streaming
cat ./output-streaming/*
```
