---
layout: post
title:  "推荐系统评价指标"
categories: 推荐系统
tags:   推荐系统 分类 CTR
author: canisn
---

* content
{:toc}




# 1. 混淆矩阵

| pred/label | Positive | Negative |
| :--------: | :------: | :------: |
|  Positive  |    TP    |    FP    |
|  Negative  |    FN    |    TN    |

**TP（true positive）**：表示样本的真实类别为正，最后预测得到的结果也为正； 

**FP（false positive）**：表示样本的真实类别为负，最后预测得到的结果却为正；

 **FN（false negative）**：表示样本的真实类别为正，最后预测得到的结果却为负； 

**TN（true negative）**：表示样本的真实类别为负，最后预测得到的结果也为负.

# 2. 准确率、精确率/召回率、F1值

**准确率：**表示分类正确的样本占总样本的比例。（正正+负负）/all

**精确率：**表示预测为正的样本中实际为正的样本数。即正正/（正正+负正）

**召回率：**表示正样本被预测为正的概率。即正正/正

**F1值：**用于平衡精确率与召回率结果，避免样本倾斜问题。

F1=2 * (召回率 * 精确率) / (召回率 + 精确率)

# 3. AUC

AUC值表示的是ROC曲线与x轴包围面积大小

其中：ROC曲线的横轴表示假阳率，即预测为正的样本中预测错误的比例，或者说样本为负，预测为正的概率。

​				纵轴表示真阳率，即预测为正的样本中预测正确的比例， 或者说样本为正，预测为正的概率。

AUC还有另一种解释，就是**测试任意给一个正类样本和一个负类样本，正类样本的score有多大的概率大于负类样本的score**。

弄明白了AUC的原理，理解上面这些话都没有问题，但是不理解还是比较费劲，下面认真解释。

啊画图好麻烦再说吧

# 4.Mean Average Precision(MAP)

在了解MAP(Mean Average Precision)之前，先来看一下AP(Average Precision), 即为平均准确率。

对于AP可以用这种方式理解: 假使当我们使用google搜索某个关键词，返回了10个结果。当然最好的情况是这10个结果都是我们想要的相关信息。但是假如只有部分是相关的，比如5个，那么这5个结果如果被显示的比较靠前也是一个相对不错的结果。但是如果这个5个相关信息从第6个返回结果才开始出现，那么这种情况便是比较差的。这便是AP所反映的指标，与recall的概念有些类似，不过是“**顺序敏感**的recall”。